{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Converting Take All Documents into JSON\r\n",
    "\r\n",
    "# Overview\r\n",
    "\r\n",
    "This Jupyter Notebook takes in translations of the Take One brochure and outputs it as a JSON file for the MyBus tool.\r\n",
    "\r\n",
    "The data was originally in a Word document.  In transferring it to a Word document, line breaks and spaces were cleaned up in the content.  Different languages use spaces differently.\r\n",
    "\r\n",
    "The output file is used on the \"All Changes\" page of the MyBus tool to display the Take One brochure as an HTML page instead of only as a PDF file.  It contains all the details for all line changes aggregated into a single view.\r\n",
    "\r\n",
    "# Notes\r\n",
    "\r\n",
    "## Not All Lines\r\n",
    "\r\n",
    "Not all lines are listed in the Take One brochure, only those with major changes.  Some lines not listed in the brochure will still have updated schedules due to minor changes.  For the All Changes page to also act as a central source for updated schedule PDFs, this data needed to be updated.\r\n",
    "\r\n",
    "## Line Numbers\r\n",
    "\r\n",
    "Lines with sister routes are listed in the brochure as a combined line.  For example - the 16/17.  To match entries with their corresponding schedule PDFs, an additional field for the line number was added."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "## Setup \r\n",
    "### 1.1 Import modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \r\n",
    "from docx.api import Document"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Read .docx and set final output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "document = Document('data/202109shakeup.docx')\r\n",
    "table = document.tables[0]\r\n",
    "\r\n",
    "headers = [\"section\",\"order\",\"line\",\"altline\",\"en\",\"es\",\"zh-TW\",\"vi\",\"ko\",\"ja\",\"hy\",\"ru\",\"new-schedule\",\"current-schedule\"]\r\n",
    "\r\n",
    "def reset_final_df():\r\n",
    "    return pd.DataFrame(columns=headers)\r\n",
    "\r\n",
    "final_df = pd.DataFrame(columns=headers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Set dataframe to docx table and pre-process data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "document = Document('data/202109shakeup.docx')\r\n",
    "table = document.tables[0]\r\n",
    "data = [[cell.text.replace(\"\\n\",\" \").replace('\"','').replace('\" ','').lstrip() for cell in row.cells] for row in table.rows]\r\n",
    "\r\n",
    "df = pd.DataFrame(data)\r\n",
    "new_header = df.iloc[0]\r\n",
    "df = df[1:] \r\n",
    "df.columns = new_header\r\n",
    "# print(df.columns)\r\n",
    "df = df.rename(columns={'English':'en','Spanish':'es','Chinese (Traditional)':'zh-TW','Korean':'ko','Vietnamese':'vi','Japanese':'ja','Russian':'ru','Armenian':'hy'})\r\n",
    "# df = df.rename(columns=df.iloc[0]).drop(df.index[0]).reset_index(drop=True)\r\n",
    "\r\n",
    "df = df.replace(' +',r' ',regex=True)\r\n",
    "df = df.replace('\"',r'',regex=True)\r\n",
    "# df.to_json('test.json')\r\n",
    "# df.to_csv('test.csv')\r\n",
    "df.head()\r\n",
    "\r\n",
    "final_df = pd.DataFrame(columns=[\"section\",\"order\",\"line\",\"altline\",\"en\",\"es\",\"zh-TW\",\"vi\",\"ko\",\"ja\",\"hy\",\"ru\",\"new-schedule\",\"current-schedule\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Populating the data\r\n",
    "\r\n",
    "### 2.1 Adding the `Summary` sections"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "header1 = df.loc[(df['en'].str.contains('\\u2013') == False) & (df['en'].str.contains('Metro is making service'))]\r\n",
    "header1 = header1.assign(section='header')\r\n",
    "header1 = header1.assign(order='1')\r\n",
    "\r\n",
    "header2 = df.loc[(df['en'].str.contains('\\u2013') == False) & (df['en'].str.contains('New schedules start'))]\r\n",
    "header2 = header2.assign(section='header')\r\n",
    "header2 = header2.assign(order='2')\r\n",
    "\r\n",
    "if not final_df.empty:\r\n",
    "    final_df = reset_final_df()\r\n",
    "\r\n",
    "final_df = final_df.append(header1)\r\n",
    "final_df = final_df.append(header2)\r\n",
    "\r\n",
    "final_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Adding pre-header for `details`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# th = df[df['en'].str.contains('Starting on'):df['en'].str.contains('We’re ')]\r\n",
    "th = df.loc[(df['en'].str.contains('\\u2013') == False) & (df.index < 30) & (df['en'].str.contains('We’re modify') == False)]\r\n",
    "\r\n",
    "th = th.assign(section='summary')\r\n",
    "\r\n",
    "th['order'] = ''\r\n",
    "\r\n",
    "th_count = th.shape[0]\r\n",
    "for i in range(0,th_count):\r\n",
    "    th['order'].values[i] = i\r\n",
    "\r\n",
    "th\r\n",
    "\r\n",
    "final_df = final_df.append(th)\r\n",
    "final_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "detail_header = df.loc[(df['en'].str.contains('\\u2013') == False) & (df.index < 20) & (df['en'].str.contains('We’re modify') == True)]\r\n",
    "\r\n",
    "detail_header = detail_header.assign(section='details')\r\n",
    "detail_header = detail_header.assign(order=0)\r\n",
    "\r\n",
    "final_df = final_df.append(detail_header)\r\n",
    "detail_header\r\n",
    "# final_df.to_json('final_takeone.json',orient='records')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "detail_header"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Adding the `details`/lines section\r\n",
    "\r\n",
    "#### 2.3.1 Process all the lines\r\n",
    "First we will read all the lines in from the master list of all the lines."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lines_df = pd.read_csv('data/mybus-sep-2021 - Lines.csv', index_col=0)\r\n",
    "lines_df['AltLine'] = lines_df.AltLine.fillna(0).astype(int)\r\n",
    "all_lines = lines_df[['Line Label',\"AltLine\"]]\r\n",
    "\r\n",
    "lines_count = all_lines.shape[0]\r\n",
    "\r\n",
    "all_lines['order'] = ''\r\n",
    "\r\n",
    "for i in range(0,lines_count):\r\n",
    "    all_lines['order'].values[i] = i+1\r\n",
    "all_lines.reset_index(inplace=True)\r\n",
    "all_lines = all_lines.rename(columns={\"Line Label\":\"line_label\",\"Line Number\":\"line\"})\r\n",
    "all_lines.head(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2 Filter the docx table for the `line details`\r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### filter the lines out based on em-dash and rail lines\r\n",
    "lines_takeone_df = df.loc[(df['en'].str.contains('\\u2013')) & (df['en'].str.contains('B Line, D Line') == False)]\r\n",
    "\r\n",
    "### create a field called `line` and set it to the first part of the split `em-dash`\r\n",
    "lines_takeone_df['line'] = lines_takeone_df.en.str.split('–').str[0]\r\n",
    "\r\n",
    "### extract duplicates\r\n",
    "lines_takeone_df = lines_takeone_df.assign(oid=lines_takeone_df.line.str.split('/')).explode('oid')\r\n",
    "dupes = lines_takeone_df.loc[(lines_takeone_df.duplicated(subset=['line']))]\r\n",
    "\r\n",
    "### remove duplicates\r\n",
    "lines_takeone_df = lines_takeone_df.drop_duplicates(subset=['line'])\r\n",
    "\r\n",
    "### remove any lines with the \"/\" in it\r\n",
    "lines_takeone_df = lines_takeone_df[lines_takeone_df[\"line\"].str.contains(\"/\")==False]\r\n",
    "\r\n",
    "lines_takeone_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.3 Re-add duplicates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dupes['line'] = dupes['line'].str.split('/')\r\n",
    "dupes = dupes.explode('line')\r\n",
    "\r\n",
    "lines_takeone_df = lines_takeone_df.append(dupes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.4 Join `lines docx` data to `all lines` data\r\n",
    "\r\n",
    "We use the pandas method `merge` to join the data on the `line` field and use an `outer` join to make sure to keep all the line data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### convert the unique line field to the same data type, integers \r\n",
    "all_lines['line'] = all_lines['line'].astype(int)\r\n",
    "lines_takeone_df['line'] = lines_takeone_df['line'].astype(int)\r\n",
    "\r\n",
    "### perform the merge \r\n",
    "merged_lines = all_lines.merge(lines_takeone_df, on='line',how='outer')\r\n",
    "\r\n",
    "### assign the \"details\" section\r\n",
    "merged_lines = merged_lines.assign(section='details')\r\n",
    "\r\n",
    "merged_lines"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.5 Join the merged lines to the final data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df = final_df.append(merged_lines)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.6 Join the rail data at the end of the `details` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### filter out the rail lines\r\n",
    "### note: right now this is hard coded... need a list of rail lines..\r\n",
    "rail_df = df.loc[(df['en'].str.contains('\\u2013')) & (df['en'].str.contains('B Line, D Line') == True)]\r\n",
    "\r\n",
    "### add this to the end of all the lines\r\n",
    "end_lines = len(merged_lines) +1\r\n",
    "\r\n",
    "### set the properties\r\n",
    "rail_df = rail_df.assign(section='details')\r\n",
    "rail_df = rail_df.assign(order=end_lines)\r\n",
    "\r\n",
    "### add to the final data frame\r\n",
    "final_df = final_df.append(rail_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Add the `end` section"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### process the first end section\r\n",
    "end1 = df.loc[(df['en'].str.contains('For more information '))]\r\n",
    "end1 = end1.assign(section='end')\r\n",
    "end1 = end1.assign(order=1)\r\n",
    "\r\n",
    "### process the second end section\r\n",
    "end2 = df.loc[(df['en'].str.contains('\\\\* M'))]\r\n",
    "end2 = end2.assign(order=2)\r\n",
    "end2 = end2.assign(section='end')\r\n",
    "\r\n",
    "### add the second end section to the first\r\n",
    "end1 = end1.append(end2)\r\n",
    "\r\n",
    "### add the end section to the final data frame\r\n",
    "final_df = final_df.append(end1)\r\n",
    "\r\n",
    "### preview the end section\r\n",
    "end1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final output\r\n",
    "### 3.1 Check the data frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Split the final data frame into JSON files depending on the language"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "languages = ['en','es','zh-TW','vi','ko','ja','hy','ru']\r\n",
    "DATA_OUTPUT_PATH = \"./data/\"\r\n",
    "for i in languages:\r\n",
    "    final_final_df = final_df[['section','order', i,'line', 'new-schedule', 'current-schedule']].copy()\r\n",
    "    final_final_df = final_final_df.rename(columns={i: 'content'})\r\n",
    "    final_final_df.to_json(DATA_OUTPUT_PATH + 'takeone-' + i + '.json',orient='records')\r\n",
    "    print('Takeone created for: ' + i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extra code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### RIP: code to split based on `:`\r\n",
    "# th['en'] = th['en'].str.split(':')\r\n",
    "# th = th.explode('en')\r\n",
    "###"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('metro': conda)"
  },
  "interpreter": {
   "hash": "7c9313544d03373dbd8584603e7825c146326b23d229fda91fe5c8d2c1173eaa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}